{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77669632-f212-4bf0-828b-da585aa77dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE criteo_x1/\n",
      "                           PRE movielens/\n",
      "                           PRE output/\n",
      "                           PRE schema/\n",
      "                           PRE test/\n",
      "                           PRE tianchi/\n",
      "                           PRE train/\n",
      "                           PRE tuner/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dmetasoul-bucket/demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9fb33b-c7bd-401a-b1e6-0c632f9d70b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE config/\n",
      "                           PRE feature_generation/\n",
      "                           PRE mango/\n",
      "                           PRE match/\n",
      "                           PRE ml-1m/\n",
      "                           PRE model/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dmetasoul-bucket/demo/movielens/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4148d433-f0fb-45c6-82b6-044d16931162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://dmetasoul-bucket/demo/movielens/feature_generation/num_negs_200/train.parquet\n",
      "s3://dmetasoul-bucket/demo/movielens/feature_generation/num_negs_200/test.parquet\n",
      "s3://dmetasoul-bucket/demo/movielens/feature_generation/num_negs_200/item.parquet\n"
     ]
    }
   ],
   "source": [
    "num_negs = 200\n",
    "\n",
    "movies_path='s3://dmetasoul-bucket/demo/movielens/ml-1m/movies.dat'\n",
    "ratings_path='s3://dmetasoul-bucket/demo/movielens/ml-1m/ratings.dat'\n",
    "users_path='s3://dmetasoul-bucket/demo/movielens/ml-1m/users.dat'\n",
    "\n",
    "train_dataset_out_path='s3://dmetasoul-bucket/demo/movielens/feature_generation/num_negs_%d/train.parquet'%num_negs\n",
    "test_dataset_out_path='s3://dmetasoul-bucket/demo/movielens/feature_generation/num_negs_%d/test.parquet'%num_negs\n",
    "item_dataset_out_path='s3://dmetasoul-bucket/demo/movielens/feature_generation/num_negs_%d/item.parquet'%num_negs\n",
    "print(train_dataset_out_path)\n",
    "print(test_dataset_out_path)\n",
    "print(item_dataset_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e394d156-f68a-4613-95f0-398b6e19746b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/01/28 12:05:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.2\n",
      "spark-application-1643371543426\n",
      "http://jupyter.my.nginx.test/hub/user-redirect/proxy/4040/jobs/\n"
     ]
    }
   ],
   "source": [
    "import metaspore as ms\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType, StringType\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName('Feature generation - movielens')\n",
    "    .config(\"spark.executor.memory\",\"5G\")\n",
    "    .config(\"spark.executor.instances\",\"4\")\n",
    "    .config(\"spark.network.timeout\",\"500\") # 500s\n",
    "    #.config(\"spark.kubernetes.executor.deleteOnTermination\", \"false\")\n",
    "    #.config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"512\")\n",
    "    #.config(\"spark.submit.pyFiles\", \"python.zip\")\n",
    "    .getOrCreate())\n",
    "    \n",
    "sc = spark.sparkContext\n",
    "print(sc.version)\n",
    "print(sc.applicationId)\n",
    "print(sc.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d377ecc-7525-49ba-81f3-44848d451dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|movie_id|               title|               genre|\n",
      "+--------+--------------------+--------------------+\n",
      "|       1|    Toy Story (1995)|Animation|Childre...|\n",
      "|       2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|       3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|       4|Waiting to Exhale...|        Comedy|Drama|\n",
      "|       5|Father of the Bri...|              Comedy|\n",
      "|       6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|       7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|       8| Tom and Huck (1995)|Adventure|Children's|\n",
      "|       9| Sudden Death (1995)|              Action|\n",
      "|      10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "item profile sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===============================================>       (172 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|movie_id|               title|               genre|\n",
      "+--------+--------------------+--------------------+\n",
      "|       1|    Toy Story (1995)|Animation|Childre...|\n",
      "|       2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|       3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|       4|Waiting to Exhale...|        Comedy|Drama|\n",
      "|       5|Father of the Bri...|              Comedy|\n",
      "|       6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|       7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|       8| Tom and Huck (1995)|Adventure|Children's|\n",
      "|       9| Sudden Death (1995)|              Action|\n",
      "|      10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### read movies\n",
    "movies_schema = StructType([\n",
    "        StructField(\"movie_id\", LongType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"genre\", StringType(), True)\n",
    "])\n",
    "\n",
    "movies = spark.read.csv(movies_path, sep='::',inferSchema=False, header=False, schema=movies_schema)\n",
    "print('movies sample:')\n",
    "movies.show(10)\n",
    "\n",
    "\n",
    "item_profile=movies.select('movie_id', 'title', 'genre').dropDuplicates(['movie_id'])\n",
    "print('item profile sample:')\n",
    "item_profile.sort('movie_id').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "476b73ec-d191-49a7-9779-4015fd8f7330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---------+\n",
      "|user_id|movie_id|rating|timestamp|\n",
      "+-------+--------+------+---------+\n",
      "|      1|    1193|   5.0|978300760|\n",
      "|      1|     661|   3.0|978302109|\n",
      "|      1|     914|   3.0|978301968|\n",
      "|      1|    3408|   4.0|978300275|\n",
      "|      1|    2355|   5.0|978824291|\n",
      "|      1|    1197|   3.0|978302268|\n",
      "|      1|    1287|   5.0|978302039|\n",
      "|      1|    2804|   5.0|978300719|\n",
      "|      1|     594|   4.0|978302268|\n",
      "|      1|     919|   4.0|978301368|\n",
      "+-------+--------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### read ratings\n",
    "ratings_schema = StructType([\n",
    "        StructField(\"user_id\", LongType(), True),\n",
    "        StructField(\"movie_id\", LongType(), True),\n",
    "        StructField(\"rating\", FloatType(), True),\n",
    "        StructField(\"timestamp\", LongType(), True)\n",
    "])\n",
    "\n",
    "ratings = spark.read.csv(ratings_path, sep='::', inferSchema=False, header=False, schema=ratings_schema)\n",
    "print('ratings sample:')\n",
    "#ratings = ratings.limit(10000) # total: 1000209\n",
    "ratings.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "056a9a21-5122-49eb-b316-564b11b8e360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users sample:\n",
      "+-------+------+---+----------+-----+\n",
      "|user_id|gender|age|occupation|  zip|\n",
      "+-------+------+---+----------+-----+\n",
      "|      1|     F|  1|        10|48067|\n",
      "|      2|     M| 56|        16|70072|\n",
      "|      3|     M| 25|        15|55117|\n",
      "|      4|     M| 45|         7|02460|\n",
      "|      5|     M| 25|        20|55455|\n",
      "|      6|     F| 50|         9|55117|\n",
      "|      7|     M| 35|         1|06810|\n",
      "|      8|     M| 25|        12|11413|\n",
      "|      9|     M| 25|        17|61614|\n",
      "|     10|     F| 35|         1|95370|\n",
      "+-------+------+---+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "user profile sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=========================================>             (150 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+----------+-----+\n",
      "|user_id|gender|age|occupation|  zip|\n",
      "+-------+------+---+----------+-----+\n",
      "|      1|     F|  1|        10|48067|\n",
      "|      2|     M| 56|        16|70072|\n",
      "|      3|     M| 25|        15|55117|\n",
      "|      4|     M| 45|         7|02460|\n",
      "|      5|     M| 25|        20|55455|\n",
      "|      6|     F| 50|         9|55117|\n",
      "|      7|     M| 35|         1|06810|\n",
      "|      8|     M| 25|        12|11413|\n",
      "|      9|     M| 25|        17|61614|\n",
      "|     10|     F| 35|         1|95370|\n",
      "+-------+------+---+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### read users\n",
    "users_schema = StructType([\n",
    "        StructField(\"user_id\", LongType(), True),\n",
    "        StructField(\"gender\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"occupation\", StringType(), True),\n",
    "        StructField(\"zip\", StringType(), True)\n",
    "])\n",
    "\n",
    "users = spark.read.csv(users_path, sep='::', inferSchema=False, header=False, schema=users_schema)\n",
    "print('users sample:')\n",
    "users.show(10)\n",
    "\n",
    "\n",
    "user_profile = users.select(\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\").dropDuplicates(['user_id'])\n",
    "print('user profile sample:')\n",
    "user_profile.sort('user_id').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "657cb7e1-6058-4bda-9cb3-28e8a801af50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset sample:\n",
      "+-------+------+---+----------+-----+--------+--------------------+--------------------+------+---------+\n",
      "|user_id|gender|age|occupation|  zip|movie_id|               title|               genre|rating|timestamp|\n",
      "+-------+------+---+----------+-----+--------+--------------------+--------------------+------+---------+\n",
      "|      1|     F|  1|        10|48067|    1193|One Flew Over the...|               Drama|   5.0|978300760|\n",
      "|      1|     F|  1|        10|48067|     661|James and the Gia...|Animation|Childre...|   3.0|978302109|\n",
      "|      1|     F|  1|        10|48067|     914| My Fair Lady (1964)|     Musical|Romance|   3.0|978301968|\n",
      "|      1|     F|  1|        10|48067|    3408|Erin Brockovich (...|               Drama|   4.0|978300275|\n",
      "|      1|     F|  1|        10|48067|    2355|Bug's Life, A (1998)|Animation|Childre...|   5.0|978824291|\n",
      "|      1|     F|  1|        10|48067|    1197|Princess Bride, T...|Action|Adventure|...|   3.0|978302268|\n",
      "|      1|     F|  1|        10|48067|    1287|      Ben-Hur (1959)|Action|Adventure|...|   5.0|978302039|\n",
      "|      1|     F|  1|        10|48067|    2804|Christmas Story, ...|        Comedy|Drama|   5.0|978300719|\n",
      "|      1|     F|  1|        10|48067|     594|Snow White and th...|Animation|Childre...|   4.0|978302268|\n",
      "|      1|     F|  1|        10|48067|     919|Wizard of Oz, The...|Adventure|Childre...|   4.0|978301368|\n",
      "+-------+------+---+----------+-----+--------+--------------------+--------------------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge movies, users, ratings\n",
    "dataset = ratings.join(users, on=ratings.user_id==users.user_id, how='leftouter').drop(users.user_id)\n",
    "dataset = dataset.join(movies, on=dataset.movie_id==movies.movie_id,how='leftouter').drop(movies.movie_id)\n",
    "dataset = dataset.select('user_id', \\\n",
    "                         'gender', \\\n",
    "                         'age', \\\n",
    "                         'occupation', \\\n",
    "                         'zip', \\\n",
    "                         'movie_id', \\\n",
    "                         'title', \\\n",
    "                         'genre', \\\n",
    "                         'rating', \\\n",
    "                         'timestamp'\n",
    "                        )\n",
    "print('dataset sample:')\n",
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b68665-a425-44bb-a7dc-4d13be3bc8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import rand\n",
    "from functools import reduce\n",
    "\n",
    "# feature generation\n",
    "def feature_generation(dataset, user_profile, item_profile, max_len=10, sep=u'\\u0001'):\n",
    "    def get_recent_items(kv_pairs, max_len=max_len):\n",
    "        # sort\n",
    "        kv_pairs.sort(key=lambda x: x[1])\n",
    "        # get recent list\n",
    "        recent_items = []\n",
    "        for i in range(0, len(kv_pairs)):\n",
    "            current, hist_time = kv_pairs[i]\n",
    "            hist_list = [0] if i == 0 else reduce(lambda x, y:x+y, map(lambda x:[x[0]], kv_pairs[:i]))\n",
    "            # get last max_len items\n",
    "            hist_list = hist_list[-max_len:]\n",
    "            hist_list = str.join(sep, map(str, hist_list))\n",
    "            recent_items.append((hist_time, current, hist_list))\n",
    "\n",
    "        return recent_items\n",
    "    \n",
    "    # label\n",
    "    dataset = dataset.withColumn('label',  F.when(F.col('rating')> 0, 1).otherwise(0))\n",
    "    \n",
    "    # generate user recent behaviors features\n",
    "    hist_item_list_df = dataset.filter(dataset['rating']>0).select('user_id','movie_id', 'timestamp').distinct().rdd\\\n",
    "                               .map(lambda x: (x['user_id'], [(x['movie_id'], x['timestamp'])]))\\\n",
    "                               .reduceByKey(lambda x, y: x + y)\\\n",
    "                               .map(lambda x: (x[0], get_recent_items(x[1])))\\\n",
    "                               .flatMapValues(lambda x: x)\\\n",
    "                               .map(lambda x: (x[0], x[1][0], x[1][1], x[1][2]))\\\n",
    "                               .toDF(['user_id', \\\n",
    "                                      'timestamp', \\\n",
    "                                      'movie_id', \\\n",
    "                                      'recent_movie_ids'])\n",
    "    # merge features\n",
    "    fg_result = dataset.alias('t1')\\\n",
    "                       .join(hist_item_list_df.alias('t2'), \\\n",
    "                             (col('t1.user_id')==col('t2.user_id')) & (col('t1.timestamp')==col('t2.timestamp')) & (col('t1.movie_id')==col('t2.movie_id')),\n",
    "                             how='leftouter')\\\n",
    "                       .select('t1.label', \\\n",
    "                               't1.user_id', \\\n",
    "                               't1.gender', \\\n",
    "                               't1.age', \\\n",
    "                               't1.occupation', \\\n",
    "                               't1.zip', \\\n",
    "                               't1.movie_id', \\\n",
    "                               't2.recent_movie_ids', \\\n",
    "                               't1.genre', \\\n",
    "                               't1.timestamp')\n",
    "    \n",
    "    # replace sep in genre column\n",
    "    fg_result = fg_result.withColumn('genre', regexp_replace('genre', '\\|', sep))\n",
    "    \n",
    "    # shuffle and return\n",
    "    # fg_result = fg_result.withColumn('rand', rand(seed=100)).orderBy('rand')\n",
    "    # fg_result = fg_result.drop('rand')\n",
    "    \n",
    "    # https://stackoverflow.com/questions/40478018/pyspark-dataframe-convert-multiple-columns-to-float\n",
    "    # fg_result = fg_result.select(*(col(c).cast('string').alias(c) for c in fg_result.columns))\n",
    "    \n",
    "    return fg_result\n",
    "\n",
    "\n",
    "fg_dataset = feature_generation(dataset, user_profile, item_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b542e8ff-bcc3-4d0f-a4dd-bcf7f5682404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, user_id: bigint, gender: string, age: int, occupation: string, zip: string, movie_id: bigint, recent_movie_ids: string, genre: string, timestamp: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train, test\n",
    "def split_train_test(dataset):\n",
    "    dataset.registerTempTable('dataset')        \n",
    "    query = \"\"\"\n",
    "    select label, user_id, gender, age, occupation, zip, movie_id, recent_movie_ids, genre, timestamp\n",
    "    from\n",
    "    (\n",
    "        select\n",
    "            *,\n",
    "            ROW_NUMBER() OVER(PARTITION BY user_id ORDER BY timestamp DESC) as sample_id\n",
    "        from\n",
    "            dataset\n",
    "    ) ta\n",
    "    where ta.sample_id = 1\n",
    "    order by user_id ASC\n",
    "    \"\"\"\n",
    "    test_dataset = spark.sql(query)\n",
    "    train_dataset = dataset.exceptAll(test_dataset)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_fg_dataset, test_fg_dataset = split_train_test(fg_dataset)\n",
    "train_fg_dataset.cache()\n",
    "test_fg_dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0fb2f3f-927e-4534-b513-93f0b8ebb2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def gen_sample_prob(dataset, group_by, alpha=0.75):\n",
    "    from pyspark.sql.functions import col, pow\n",
    "    item_weight = dataset.groupBy(col(group_by)).count()\n",
    "    item_weight = item_weight.withColumn('norm_weight', pow(item_weight['count'], alpha))\n",
    "    total_freq = item_weight.select('count').groupBy().sum().collect()[0][0]\n",
    "    total_norm = item_weight.select('norm_weight').groupBy().sum().collect()[0][0]\n",
    "    item_weight = item_weight.withColumn('sampling_prob', item_weight['norm_weight']/total_norm)    \n",
    "    return item_weight, total_norm, total_freq\n",
    "\n",
    "test_df, _, _ = gen_sample_prob(dataset, 'movie_id')\n",
    "test_dist = test_df.select('movie_id', 'sampling_prob')\\\n",
    "                            .rdd.map(lambda x: (x[0], x[1])).collect()\n",
    "zipped_dist = [list(t) for t in zip(*test_dist)]\n",
    "item_list, dist_list = zipped_dist[0], zipped_dist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6832dc05-26c6-4c63-a168-9270816832b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative sampling result size:16440817\n",
      "negative samping result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+\n",
      "|label|user_id|gender|age|occupation|  zip|movie_id|    recent_movie_ids|               genre|\n",
      "+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+\n",
      "|    0|      5|     M| 25|        20|55455|      87|1529\u00013260\u00013499\u000110...|Crime\u0001Drama\u0001Roman...|\n",
      "|    0|      5|     M| 25|        20|55455|     196|1529\u00013260\u00013499\u000110...|Crime\u0001Drama\u0001Roman...|\n",
      "|    0|      5|     M| 25|        20|55455|     214|1529\u00013260\u00013499\u000110...|Crime\u0001Drama\u0001Roman...|\n",
      "|    0|      5|     M| 25|        20|55455|     722|1529\u00013260\u00013499\u000110...|Crime\u0001Drama\u0001Roman...|\n",
      "|    0|      5|     M| 25|        20|55455|     895|1529\u00013260\u00013499\u000110...|Crime\u0001Drama\u0001Roman...|\n",
      "|    0|      5|     M| 25|        20|55455|     927|1529\u00013260\u00013499\u000110...|Crime\u0001Drama\u0001Roman...|\n",
      "|    0|      5|     M| 25|        20|55455|     969|1529\u00013260\u00013499\u000110...|Crime\u0001Drama\u0001Roman...|\n",
      "|    0|      5|     M| 25|        20|55455|    1399|1529\u00013260\u00013499\u000110...|Crime\u0001Drama\u0001Roman...|\n",
      "|    0|      5|     M| 25|        20|55455|    1703|1529\u00013260\u00013499\u000110...|Crime\u0001Drama\u0001Roman...|\n",
      "|    0|      5|     M| 25|        20|55455|    1811|1529\u00013260\u00013499\u000110...|Crime\u0001Drama\u0001Roman...|\n",
      "+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset sample size:994169\n",
      "original dataset sample:\n",
      "+-----+-------+------+---+----------+-----+--------+--------------------+----------------+\n",
      "|label|user_id|gender|age|occupation|  zip|movie_id|    recent_movie_ids|           genre|\n",
      "+-----+-------+------+---+----------+-----+--------+--------------------+----------------+\n",
      "|    1|    736|     M| 18|        12|07070|    1036|2722\u0001223\u00012599\u0001288...| Action\u0001Thriller|\n",
      "|    1|    755|     F| 35|         0|94002|    2599|2272\u00011196\u00012990\u000192...|          Comedy|\n",
      "|    1|   1260|     M| 25|        17|28262|    3100|1095\u00012395\u00012276\u000111...|           Drama|\n",
      "|    1|   1263|     M|  1|        10|81301|    2598|2805\u00013354\u00013752\u000125...|          Comedy|\n",
      "|    1|   1298|     M| 35|         6|33615|    3263|562\u0001337\u00011947\u00013809...|          Comedy|\n",
      "|    1|   1494|     M| 25|        17|38104|    1390|                   0|          Comedy|\n",
      "|    1|   1530|     M| 25|         4|53711|    1247|2371\u00013174\u0001471\u000147\u0001...|   Drama\u0001Romance|\n",
      "|    1|   1632|     M| 25|        16|94120|    1223|1188\u00012396\u00013462\u000129...|Animation\u0001Comedy|\n",
      "|    1|   1737|     M| 35|        20|46614|    1682|3044\u00012395\u00013252\u000136...|           Drama|\n",
      "|    1|   1897|     F| 50|         2|94530|    1231|1228\u00011225\u00011394\u000112...|           Drama|\n",
      "+-----+-------+------+---+----------+-----+--------+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# negative sampling on original dataset\n",
    "def negative_sampling(dataset, user_column='user_id', item_column='movie_id', time_column='timestamp', \\\n",
    "                      negative_item_column='trigger_item_id', negative_sample=3):\n",
    "    \n",
    "    def gen_sample_prob(dataset, group_by, alpha=0.75):\n",
    "        from pyspark.sql.functions import col, pow\n",
    "        item_weight = dataset.groupBy(col(group_by)).count()\n",
    "        item_weight = item_weight.withColumn('norm_weight', pow(item_weight['count'], alpha))\n",
    "        total_freq = item_weight.select('count').groupBy().sum().collect()[0][0]\n",
    "        total_norm = item_weight.select('norm_weight').groupBy().sum().collect()[0][0]\n",
    "        item_weight = item_weight.withColumn('sampling_prob', item_weight['norm_weight']/total_norm)    \n",
    "        return item_weight, total_norm, total_freq\n",
    "    \n",
    "    def sample(user_id, user_item_list, item_list, dist_list, negative_sample):\n",
    "        import numpy as np\n",
    "        # sample negative list\n",
    "        candidate_list = np.random.choice(list(item_list), size=len(user_item_list)*negative_sample, \\\n",
    "                                          replace=True, p=dist_list).tolist()\n",
    "        # remove the positive sample from the sampling result\n",
    "        candidate_list = list(set(candidate_list)-set(user_item_list))\n",
    "        \n",
    "        # sample trigger list\n",
    "        trigger_list = np.random.choice(list(user_item_list), size=len(candidate_list), \\\n",
    "                                        replace=True).tolist()\n",
    "        \n",
    "        return list(zip(trigger_list, candidate_list))\n",
    "    \n",
    "    # sampling distribution\n",
    "    item_weight, _, _ = gen_sample_prob(dataset, item_column)\n",
    "    item_list = item_weight.select(item_column).rdd.flatMap(lambda x: x).collect()\n",
    "    dist_list = item_weight.select('sampling_prob').rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # generate sampling dataframe\n",
    "    sampling_df=dataset.rdd\\\n",
    "                       .map(lambda x: (x[user_column], [x[item_column]]))\\\n",
    "                       .reduceByKey(lambda x, y: x + y)\\\n",
    "                       .map(lambda x: (x[0], sample(x[0], x[1], item_list, dist_list, negative_sample)))\\\n",
    "                       .flatMapValues(lambda x: x)\\\n",
    "                       .map(lambda x: (x[0], x[1][0], x[1][1]))\\\n",
    "                       .toDF([user_column, negative_item_column, item_column])\n",
    "    \n",
    "    return sampling_df\n",
    "\n",
    "# negative sampling\n",
    "neg_sample_df=negative_sampling(dataset=train_fg_dataset, user_column='user_id', item_column='movie_id', time_column='timestamp', negative_sample=num_negs)\n",
    "\n",
    "# merge into item and user profile information\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col\n",
    "neg_sample_df = neg_sample_df.withColumn('label', lit(0))\n",
    "neg_sample_df = neg_sample_df.alias('t1')\\\n",
    "                        .join(train_fg_dataset.alias('t2'), \\\n",
    "                             (col('t1.user_id')==col('t2.user_id')) & (col('t1.trigger_item_id')==col('t2.movie_id')),\n",
    "                             how='leftouter')\\\n",
    "                        .select('t1.label', \\\n",
    "                                't1.user_id', \\\n",
    "                                't2.gender', \\\n",
    "                                't2.age', \\\n",
    "                                't2.occupation', \\\n",
    "                                't2.zip', \\\n",
    "                                't1.movie_id', \\\n",
    "                                't2.recent_movie_ids', \\\n",
    "                                't2.genre')\n",
    "\n",
    "# show negative sampling result\n",
    "print('negative sampling result size:%d'%neg_sample_df.count())\n",
    "print('negative samping result:')\n",
    "neg_sample_df.show(10)\n",
    "\n",
    "# show origianl dataset\n",
    "train_fg_dataset = train_fg_dataset.drop('timestamp')\n",
    "print('original dataset sample size:%d'%train_fg_dataset.count())\n",
    "print('original dataset sample:')\n",
    "train_fg_dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0080632c-48c9-481f-8052-c7800ad7bb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: string, user_id: string, gender: string, age: string, occupation: string, zip: string, movie_id: string, recent_movie_ids: string, genre: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_fg_dataset.union(neg_sample_df)\n",
    "train_dataset = train_dataset.withColumn('rand', rand(seed=100)).orderBy('rand')\n",
    "train_dataset = train_dataset.drop('rand')\n",
    "train_dataset = train_dataset.select(*(col(c).cast('string').alias(c) for c in train_dataset.columns))\n",
    "train_dataset.cache()\n",
    "\n",
    "test_dataset = test_fg_dataset.withColumn('rand', rand(seed=100)).orderBy('rand')\n",
    "test_dataset = test_dataset.drop('rand')\n",
    "test_dataset = test_dataset.drop('timestamp')\n",
    "test_dataset = test_dataset.select(*(col(c).cast('string').alias(c) for c in test_dataset.columns))\n",
    "test_dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e84e445-c10a-4632-9aec-d73a6468c2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train dataset size: 17434453\n",
      "final train dataset sample:\n",
      "+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+\n",
      "|label|user_id|gender|age|occupation|  zip|movie_id|    recent_movie_ids|               genre|\n",
      "+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+\n",
      "|    0|   6030|     M| 25|        17|32618|    2749|1206\u0001968\u00012664\u0001127...|Action\u0001Adventure\u0001...|\n",
      "|    0|   4547|     M| 18|        12|02115|     100|1210\u0001480\u00012858\u0001270...|               Drama|\n",
      "|    0|   5087|     F| 25|         6|19102|    2295|954\u0001589\u00011947\u0001110\u0001...|    Children's\u0001Drama|\n",
      "|    0|   4387|     F| 18|         4|63109|    3089|1961\u00012194\u0001971\u0001139...|        Comedy\u0001Drama|\n",
      "|    0|   5950|     M| 25|         4|19713|       7|1265\u00011956\u0001300\u0001129...|      Crime\u0001Thriller|\n",
      "|    0|   5573|     F| 35|         1|14619|     341|1964\u00013044\u0001800\u0001317...|Action\u0001Mystery\u0001Th...|\n",
      "|    0|   1037|     M| 45|         7|02081|    2111|3481\u00013408\u00013189\u000133...|              Comedy|\n",
      "|    0|    602|     F| 56|         6|14612|    3304|1544\u00013926\u00013660\u000116...|       Comedy\u0001Sci-Fi|\n",
      "|    0|   1884|     M| 45|        20|93108|    3718|2847\u00012927\u00012677\u000136...|               Drama|\n",
      "|    0|    385|     M| 25|         6|68131|    2638|2987\u000139\u00012890\u00013646...| Action\u0001Crime\u0001Sci-Fi|\n",
      "+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167:===================================================> (196 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test dataset size: 6040\n",
      "final test dataset sample:\n",
      "+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+\n",
      "|label|user_id|gender|age|occupation|  zip|movie_id|    recent_movie_ids|               genre|\n",
      "+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+\n",
      "|    1|   5964|     M| 18|         5|97202|    2568|429\u0001520\u0001442\u00013113\u0001...|        Action\u0001Crime|\n",
      "|    1|   4159|     F| 50|         9|01450|    2701|1\u00012396\u00012355\u0001593\u00013...|Action\u0001Sci-Fi\u0001Wes...|\n",
      "|    1|   1715|     M| 25|        16|53406|    2769|3897\u00013898\u00013946\u000139...|       Crime\u0001Mystery|\n",
      "|    1|    948|     M| 56|        12|43056|    3616|996\u0001553\u0001458\u0001590\u00013...|      Comedy\u0001Romance|\n",
      "|    1|    920|     M| 18|         4|92173|    1779|380\u00011356\u0001590\u0001316\u0001...|Adventure\u0001Sci-Fi\u0001...|\n",
      "|    1|   2052|     M|  1|        10|46033|    3016|2827\u00012997\u00012710\u000126...|              Horror|\n",
      "|    1|   3214|     F| 56|         7|10019|    2388|1694\u00011208\u0001952\u0001191...|       Drama\u0001Romance|\n",
      "|    1|   4354|     M| 35|        14|78611|     923|3741\u00013735\u00011254\u000112...|               Drama|\n",
      "|    1|   2680|     M| 25|        12|01864|      86|2491\u00012762\u00012670\u000133...|     Adventure\u0001Drama|\n",
      "|    1|     53|     M| 25|         0|96931|     581|2421\u00011522\u0001193\u0001141...|         Documentary|\n",
      "+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# show results\n",
    "print('final train dataset size: %d'%train_dataset.count())\n",
    "print('final train dataset sample:')\n",
    "train_dataset.show(10)\n",
    "\n",
    "print('final test dataset size: %d'%test_dataset.count())\n",
    "print('final test dataset sample:')\n",
    "test_dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba600726-b423-4685-ae4b-688564777020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 196:============================================>        (167 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final item dataset size: 3706\n",
      "final item dataset sample:\n",
      "+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+\n",
      "|label|user_id|gender|age|occupation|  zip|movie_id|    recent_movie_ids|               genre|\n",
      "+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+\n",
      "|    1|   3272|     M| 35|         0|08330|    1090|969\u000150\u0001318\u00011507\u00011...|           Drama\u0001War|\n",
      "|    1|   5762|     F| 35|         6|55125|    1436|953\u0001111\u0001971\u00011302\u0001...|              Comedy|\n",
      "|    1|   3299|     F| 25|         4|19119|    1572|955\u0001919\u0001954\u00012366\u0001...|               Drama|\n",
      "|    1|   1812|     F| 25|        12|48103|    2069|999\u00013186\u00011036\u0001261...|               Drama|\n",
      "|    1|   1671|     M| 35|         0|98368|    2088|963\u00012746\u00013199\u0001195...|Adventure\u0001Comedy\u0001...|\n",
      "|    1|   2304|     M| 45|        12|94103|    2136|910\u00013088\u00011948\u0001127...|              Comedy|\n",
      "|    1|   4568|     F| 25|         4|90034|    2162|926\u00012262\u0001497\u00011923...|Adventure\u0001Childre...|\n",
      "|    1|   5433|     F| 35|        17|45014|    2294|97\u0001365\u00011063\u00011459\u0001...|Animation\u0001Children's|\n",
      "|    1|   3610|     M| 18|         6|30064|    2904|961\u0001936\u00011340\u00012208...|               Drama|\n",
      "|    1|   5458|     F| 18|         2|98102|     296|999\u0001590\u00013090\u00012580...|         Crime\u0001Drama|\n",
      "+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_table = train_dataset.union(test_dataset).where(train_dataset['label'] == '1').distinct()\n",
    "temp_table.registerTempTable('temp_table')        \n",
    "query = \"\"\"\n",
    "select\n",
    "    label, user_id, gender, age, occupation, zip, movie_id, recent_movie_ids, genre\n",
    "from\n",
    "(\n",
    "    select\n",
    "        *,\n",
    "        ROW_NUMBER() OVER(PARTITION BY movie_id ORDER BY recent_movie_ids DESC) as sample_id\n",
    "    from\n",
    "        temp_table\n",
    ") ta\n",
    "where \n",
    "    sample_id=1\n",
    "\"\"\"\n",
    "item_dataset=spark.sql(query)\n",
    "item_dataset.cache()\n",
    "print('final item dataset size: %d'%item_dataset.count())\n",
    "print('final item dataset sample:')\n",
    "item_dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d882572-1010-4d8f-ad77-7ffe24e49184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#fg_result.coalesce(1).write.option(\"header\", True).option('sep','::').csv(out_path, mode=\"overwrite\")\n",
    "train_dataset.write.parquet(train_dataset_out_path, mode=\"overwrite\")\n",
    "test_dataset.write.parquet(test_dataset_out_path, mode=\"overwrite\")\n",
    "item_dataset.write.parquet(item_dataset_out_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e84e8-7a90-4dce-a952-f7b3aefdcaea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c26f2-7452-499b-b991-03b940d9e0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90ccf152-29a8-432e-bbd4-2ba71372b03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE item.parquet/\n",
      "                           PRE test.parquet/\n",
      "                           PRE train.parquet/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dmetasoul-bucket/demo/movielens/feature_generation/num_negs_3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b83480b3-8054-4eb4-9e10-4fedd19f9e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = spark.read.csv(out_path, sep='::', header=True)\n",
    "train_temp = spark.read.parquet(train_dataset_out_path)\n",
    "test_temp = spark.read.parquet(test_dataset_out_path)\n",
    "item_temp = spark.read.parquet(item_dataset_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16025b88-878b-429f-bc9b-a4bc63e66cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17434453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 263:================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(train_temp.count())\n",
    "print(test_temp.count())\n",
    "print(item_temp.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "248c3bc4-d76d-4b3b-97ee-aea0731f96dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/21 08:24:55 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b7c7b-1e0e-490f-86f8-8339ef55eec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b65327d-c89b-4619-b1d2-ea22d99d5cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb719e-9bd5-478f-a431-e530d9e25846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d41d007d-7e81-45e6-aae3-5b7f84c88b35",
   "metadata": {},
   "source": [
    "# Debug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7788bd55-e4d3-4db2-8d46-27f6833470f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.cache()\n",
    "dataset.show(5)\n",
    "dataset.groupBy(dataset.user_id).count().orderBy(['count'], ascending=[1]).show()\n",
    "print(dataset.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f651f-da95-4a27-a6e9-69c48eadbd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.choice(dist_list, size=5, \\\n",
    "                                          replace=True, p=dist_list).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4b294-06fd-4a25-bab5-9dd70a586ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
