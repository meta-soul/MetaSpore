{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd6630-1a7a-4a88-add2-dd41f1672a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark.sql import Window, functions as F\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3139a300-2f3f-4c94-8c1a-0e3f582ed286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import metaspore as ms\n",
    "\n",
    "spark_confs={\n",
    "        \"spark.network.timeout\":\"500\",\n",
    "        \"spark.sql.codegen.wholeStage\": \"false\"\n",
    "    }\n",
    "\n",
    "spark_session = ms.spark.get_session(local=True,\n",
    "                             app_name='soc-pokec Demo',\n",
    "                             batch_size=256,\n",
    "                             worker_count=2,\n",
    "                             server_count=2,\n",
    "                             worker_memory='10G',\n",
    "                             server_memory='10G',\n",
    "                             coordinator_memory='10G',\n",
    "                             spark_confs=spark_confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31728172-fb7e-439c-bac6-0eb0523e7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://dmetasoul-bucket/demo/datasets/soc-pokec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305ff81-6e72-46a1-bc7b-ddda152c409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "profile_colunm_names = ['user_id', 'public', 'completion_percentage', 'gender', 'region', 'last_login', 'registration',\n",
    "                'AGE', 'body', 'I_am_working_in_field', 'spoken_languages', 'hobbies', 'I_most_enjoy_good_food',\n",
    "                'pets', 'body_type', 'my_eyesight', 'eye_color', 'hair_color', 'hair_type', 'completed_level_of_education',\n",
    "                'favourite_color', 'relation_to_smoking', 'relation_to_alcohol', 'sign_in_zodiac',\n",
    "                'on_pokec_i_am_looking_for', 'love_is_for_me', 'relation_to_casual_sex', 'my_partner_should_be',\n",
    "                'marital_status', 'children', 'relation_to_children', 'I_like_movies', 'I_like_watching_movie',\n",
    "                'I_like_music', 'I_mostly_like_listening_to_music', 'the_idea_of_good_evening', 'I_like_specialties_from_kitchen',\n",
    "                'fun', 'I_am_going_to_concerts', 'my_active_sports', 'my_passive_sports', 'profession', 'I_like_books',\n",
    "                'life_style', 'music', 'cars', 'politics', 'relationships', 'art_culture', 'hobbies_interests',\n",
    "                'science_technologies', 'computers_internet', 'education', 'sport', 'movies', 'travelling', 'health',\n",
    "                'companies_brands', 'more']\n",
    "relationship_colunm_names = ['user_id', 'friend_id']\n",
    "\n",
    "profile_schema = StructType([StructField(cn, StringType(), True) for cn in profile_colunm_names])\n",
    "relationship_schema = StructType([StructField(cn, LongType(), True) for cn in relationship_colunm_names])\n",
    "\n",
    "profile_dataset = spark_session.read.csv('s3://dmetasoul-bucket/demo/datasets/soc-pokec/soc-pokec-profiles.txt', sep='\\t', schema=profile_schema, header=False, inferSchema=False)\n",
    "relationship_dataset = spark_session.read.csv('s3://dmetasoul-bucket/demo/datasets/soc-pokec/soc-pokec-relationships.txt', sep='\\t', schema=relationship_schema, header=False, inferSchema=False)\n",
    "\n",
    "profile_dataset = profile_dataset.withColumn('user_id', F.col('user_id').cast(LongType()))\n",
    "profile_dataset = profile_dataset.orderBy(F.col('user_id')).limit(16000)\n",
    "max_user_id = profile_dataset.agg({\"user_id\": \"max\"}).collect()[0]['max(user_id)']\n",
    "relationship_dataset = relationship_dataset.filter((F.col('user_id') <= max_user_id) & (F.col('friend_id') <= max_user_id))\n",
    "\n",
    "profile_dataset = profile_dataset.withColumn('user_id', F.col('user_id').cast(StringType()))\n",
    "relationship_dataset = relationship_dataset.withColumn('user_id', F.col('user_id').cast(StringType()))\n",
    "relationship_dataset = relationship_dataset.withColumn('friend_id', F.col('friend_id').cast(StringType()))\n",
    "\n",
    "profile_dataset.cache()\n",
    "relationship_dataset.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e45c07-90e7-4371-aad9-b6f4fe22a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_dataset.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d63fc9-7080-4e2a-a573-73c6a3dccb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_dataset.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4d6434-35b6-41a6-b294-518fe4936e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship_df = relationship_dataset.groupby(F.col('user_id')).agg(F.collect_set(F.col('friend_id')).alias('friends'))\n",
    "# relationship_df.limit(10).toPandas()\n",
    "\n",
    "# profile_df = profile_dataset.join(relationship_df, on=profile_dataset.user_id==relationship_df.user_id, how='leftouter').drop(relationship_df.user_id)\n",
    "# profile_df.limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b1db83-86a4-409e-907f-4a898c1f5d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_df = relationship_dataset.alias('t1').join(profile_dataset.alias('t2'), on=F.col('t1.user_id')==F.col('t2.user_id'), how='leftouter') \\\n",
    "                .select(F.col('t1.*'),\n",
    "                        F.col('t2.gender').alias('user_gender'),\n",
    "                        F.col('t2.AGE').alias('user_age'),\n",
    "                        F.col('t2.completion_percentage').alias('user_completion_percentage'))\n",
    "\n",
    "relationship_df = relationship_df.alias('t1').join(profile_dataset.alias('t2'), on=F.col('t1.friend_id')==F.col('t2.user_id'), how='leftouter') \\\n",
    "                .select(F.col('t1.*'),\n",
    "                        F.col('t2.gender').alias('friend_gender'),\n",
    "                        F.col('t2.AGE').alias('friend_age'),\n",
    "                        F.col('t2.completion_percentage').alias('friend_completion_percentage'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b10614-fdc5-4fe9-ad8f-ec345572c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_df.printSchema()\n",
    "relationship_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e667980-b4d9-449a-8cfe-2b3db6603949",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_df = relationship_df.select(F.lit('1').alias('label'), '*')\n",
    "relationship_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d71a1c0-af1a-4fbe-b495-4833fbccffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = relationship_df.randomSplit([0.9, 0.1], 24)\n",
    "train_dataset, test_dataset = splits[0], splits[1]\n",
    "\n",
    "print('train dataset count: ', train_dataset.count())\n",
    "print('test dataset count: ', test_dataset.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990d96d-f940-47d2-81f0-2bf857130566",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dataset = (\n",
    "    relationship_df\n",
    "    .withColumn('rn', F.row_number().over(\n",
    "        Window.partitionBy('friend_id').orderBy(F.col('user_id'))\n",
    "    ))\n",
    "    .filter('rn == 1')\n",
    "    .drop(F.col('rn'))\n",
    ")\n",
    "\n",
    "item_dataset.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ed6c4-5266-44a2-a731-6d0d94501ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.write.parquet('s3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_fg/train_dataset.parquet', mode=\"overwrite\")\n",
    "test_dataset.write.parquet('s3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_fg/test_dataset.parquet', mode=\"overwrite\")\n",
    "item_dataset.write.parquet('s3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_fg/item_dataset.parquet', mode=\"overwrite\")\n",
    "profile_dataset.write.parquet('s3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_fg/profile_dataset.parquet', mode=\"overwrite\")\n",
    "relationship_dataset.write.parquet('s3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_fg/relationship_dataset.parquet', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088ef1f-5181-44b9-ab07-25a72b4f3fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5ad1b87-1def-49c5-9fbe-d491cf1e73f0",
   "metadata": {},
   "source": [
    "# ItemCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f67728-f03a-4da6-9565-a1b6f0746b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tigger_df = (\n",
    "    test_dataset\n",
    "    .withColumn('rn', F.row_number().over(\n",
    "        Window.partitionBy('user_id').orderBy(F.col('friend_id')))\n",
    "        )\n",
    "    .filter('rn == 1')\n",
    "    .drop(F.col('rn'))\n",
    ")\n",
    "\n",
    "label_df = (\n",
    "    test_dataset\n",
    "    .withColumn('rn', F.row_number().over(\n",
    "        Window.partitionBy('user_id').orderBy(F.col('friend_id')))\n",
    "        )\n",
    "    .filter('rn > 1')\n",
    "    .drop(F.col('rn'))\n",
    "    .groupby('user_id')\n",
    "    .agg(F.collect_list('friend_id').alias('label_friends'))\n",
    ")\n",
    "\n",
    "test_df = (\n",
    "    tigger_df.alias('t1').join(label_df.alias('t2'), on=F.col('t1.user_id')==F.col('t2.user_id'), how='rightouter')\n",
    "    .select(F.col('t1.*'),\n",
    "            F.col('t2.label_friends'))\n",
    ")\n",
    "\n",
    "test_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b2d67f-62fc-447f-9ea7-72416dd0e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/spark/work/MetaSpore/') \n",
    "from python.algos.item_cf_retrieval import ItemCFEstimator\n",
    "\n",
    "estimator = ItemCFEstimator(user_id_column_name='user_id',\n",
    "                            item_id_column_name='friend_id',\n",
    "                            behavior_column_name='label',\n",
    "                            behavior_filter_value='1',\n",
    "                            key_column_name='key',\n",
    "                            value_column_name='value',\n",
    "                            max_recommendation_count=20,\n",
    "                            debug=True)\n",
    "\n",
    "model = estimator.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f742bdaa-1ef9-4b95-881b-7fdae8cfb179",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = model.transform(test_df)\n",
    "prediction_df = prediction_df.withColumnRenamed('value', 'rec_info')\n",
    "prediction_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce844b49-887d-45aa-9653-4f10095d10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "prediction_label_rdd = prediction_df.rdd.map(lambda x:(\\\n",
    "                                [xx._1 for xx in x.rec_info] if x.rec_info is not None else [], \\\n",
    "                                 x.label_friends))\n",
    "recall_metrics = RankingMetrics(prediction_label_rdd)\n",
    "\n",
    "print(\"Debug -- Precision@20: \", recall_metrics.precisionAt(20))\n",
    "print(\"Debug -- Recall@20: \", recall_metrics.recallAt(20))\n",
    "print(\"Debug -- MAP@20: \", recall_metrics.meanAveragePrecisionAt(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78309ec8-dd54-4573-a36a-83734ac91634",
   "metadata": {},
   "source": [
    "# Swing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19575f-d424-40f0-afb2-dde2395d25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "swing_estimator = ms.SwingEstimator(user_id_column_name='user_id',\n",
    "                            item_id_column_name='friend_id',\n",
    "                            behavior_column_name='label',\n",
    "                            behavior_filter_value='1',\n",
    "                            key_column_name='key',\n",
    "                            value_column_name='value',\n",
    "                            use_plain_weight=False,\n",
    "                            smoothing_coefficient=1.0,\n",
    "                            max_recommendation_count=20)\n",
    "\n",
    "swing_model = swing_estimator.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803231d7-6274-4f4a-b5cc-2179a107faa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "swing_prediction_df = swing_model.transform(test_df)\n",
    "swing_prediction_df = swing_prediction_df.withColumnRenamed('value', 'rec_info')\n",
    "swing_prediction_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4931546-b6a2-4e9c-be68-36b461e1ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "swing_prediction_label_rdd = swing_prediction_df.rdd.map(lambda x:(\\\n",
    "                                [xx._1 for xx in x.rec_info] if x.rec_info is not None else [], \\\n",
    "                                 x.label_friends))\n",
    "swing_recall_metrics = RankingMetrics(swing_prediction_label_rdd)\n",
    "\n",
    "print(\"Debug -- Swing Precision@20: \", swing_recall_metrics.precisionAt(20))\n",
    "print(\"Debug -- Swing Recall@20: \", swing_recall_metrics.recallAt(20))\n",
    "print(\"Debug -- Swing MAP@20: \", swing_recall_metrics.meanAveragePrecisionAt(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdb33a5-4792-4335-90da-32a595137d8c",
   "metadata": {},
   "source": [
    "# TwoTowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab96af-0bdc-442e-9254-906f43d504b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import subprocess\n",
    "import sys\n",
    "import metaspore as ms\n",
    "\n",
    "model_params = dict()\n",
    "with open('conf/soc_pokec_dssm_inbatch_new.yaml', 'r') as stream:\n",
    "    model_params = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "    print('Debug -- load config: ', model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da33314-6579-4379-ac40-07353f5e60f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(['zip', '-r', '../MetaSpore/solutions/recommend/offline/social_network/python.zip', 'python'], cwd='../../../../../recommend-algos')\n",
    "spark_confs={\n",
    "    \"spark.network.timeout\":\"500\",\n",
    "    \"spark.ui.showConsoleProgress\": \"true\",\n",
    "    \"spark.kubernetes.executor.deleteOnTermination\":\"true\",\n",
    "    \"spark.submit.pyFiles\":\"python.zip\",\n",
    "}\n",
    "spark = ms.spark.get_session(local=model_params['local'],\n",
    "                             app_name=model_params['app_name'],\n",
    "                             batch_size=model_params['batch_size'],\n",
    "                             worker_count=model_params['worker_count'],\n",
    "                             server_count=model_params['server_count'],\n",
    "                             worker_memory=model_params['worker_memory'],\n",
    "                             server_memory=model_params['server_memory'],\n",
    "                             coordinator_memory=model_params['coordinator_memory'],\n",
    "                             spark_confs=spark_confs)\n",
    "sc = spark.sparkContext\n",
    "print('Debug -- spark init')\n",
    "print('Debug -- version:', sc.version)   \n",
    "print('Debug -- applicaitonId:', sc.applicationId)\n",
    "print('Debug -- uiWebUrl:', sc.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb597f-06bc-40ba-bf85-4905fca4d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append('/home/spark/work/recommend-algos')\n",
    "from python.dssm_net import UserModule, ItemModule, SimilarityModule\n",
    "from python.training import TwoTowerBatchNegativeSamplingAgent, TwoTowerBatchNegativeSamplingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6510ce66-d0a2-4995-a69e-80dc49096b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "train_dataset = spark.read.parquet(model_params['train_path'])\n",
    "test_dataset = spark.read.parquet(model_params['test_path'])\n",
    "item_dataset = spark.read.parquet(model_params['item_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdcecba-813e-443d-a921-63a182d063d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.limit(100).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe6b40-59f6-4f1c-8c98-6d038389ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## init user module, item module, similarity module\n",
    "user_module = UserModule(model_params['user_column_name'], \\\n",
    "                         model_params['user_combine_schema'], \\\n",
    "                         emb_size = model_params['vector_embedding_size'], \\\n",
    "                         alpha = model_params['ftrl_learning_rate'], \\\n",
    "                         beta = model_params['ftrl_smothing_rate'], \\\n",
    "                         l1 = model_params['ftrl_l1_regularization'], \\\n",
    "                         l2 = model_params['ftrl_l2_regularization'], \\\n",
    "                         dense_structure = model_params['dense_structure'])\n",
    "item_module = ItemModule(model_params['item_column_name'], \\\n",
    "                         model_params['item_combine_schema'], \\\n",
    "                         emb_size = model_params['vector_embedding_size'], \\\n",
    "                         alpha = model_params['ftrl_learning_rate'], \\\n",
    "                         beta = model_params['ftrl_smothing_rate'], \\\n",
    "                         l1 = model_params['ftrl_l1_regularization'], \\\n",
    "                         l2 = model_params['ftrl_l2_regularization'], \\\n",
    "                         dense_structure = model_params['dense_structure'])\n",
    "similarity_module = SimilarityModule(model_params['tau'])\n",
    "module = TwoTowerBatchNegativeSamplingModule(user_module, item_module, similarity_module)\n",
    "\n",
    "import importlib\n",
    "module_lib = importlib.import_module(model_params['two_tower_module'])\n",
    "## init estimator class\n",
    "estimator_class_ = getattr(module_lib, model_params['two_tower_estimator_class'])\n",
    "estimator = estimator_class_(module = module,\n",
    "                             item_dataset = item_dataset,\n",
    "                             item_ids_column_indices = [2],\n",
    "                             retrieval_item_count = 20,\n",
    "                             metric_update_interval = 500,\n",
    "                             agent_class = TwoTowerBatchNegativeSamplingAgent,\n",
    "                             **model_params)\n",
    "## dnn learning rate\n",
    "estimator.updater = ms.AdamTensorUpdater(model_params['adam_learning_rate'])\n",
    "## model train\n",
    "model = estimator.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80618255-042b-4286-907e-207982771f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = model.transform(test_dataset)\n",
    "print('Debug -- test result sample:')\n",
    "test_result.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13a04f-0702-4579-a8d7-c55abdf03baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "print('Debug -- test sample:')\n",
    "test_result.select('user_id', (F.posexplode('rec_info').alias('pos', 'rec_info'))).show(60)\n",
    "\n",
    "test_result[test_result['user_id']==100]\\\n",
    "            .select('user_id', (F.posexplode('rec_info').alias('pos', 'rec_info'))).show(60)\n",
    "\n",
    "## evaluation\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "prediction_label_rdd = test_result.rdd.map(lambda x:(\\\n",
    "                                        [xx.name for xx in x.rec_info] if x.rec_info is not None else [], \\\n",
    "                                        [x.friend_id]))\n",
    "\n",
    "recall_metrics = RankingMetrics(prediction_label_rdd)\n",
    "\n",
    "print(\"Debug -- Precision@20: \", recall_metrics.precisionAt(20))\n",
    "print(\"Debug -- Recall@20: \", recall_metrics.recallAt(20))\n",
    "print(\"Debug -- MAP@20: \", recall_metrics.meanAveragePrecisionAt(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1140f5-5138-4189-85e2-ff087a428ccd",
   "metadata": {},
   "source": [
    "# Negative Sampling for CTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b304c-9241-45b3-a8ad-07cda67df42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import subprocess\n",
    "import sys\n",
    "import metaspore as ms\n",
    "\n",
    "subprocess.run(['zip', '-r', '../../solutions/recommend/offline/social_network/python.zip', 'common'], cwd='../../../../demo/dataset')\n",
    "spark_confs={\n",
    "    \"spark.network.timeout\":\"500\",\n",
    "    \"spark.ui.showConsoleProgress\": \"true\",\n",
    "    \"spark.kubernetes.executor.deleteOnTermination\":\"true\",\n",
    "    \"spark.submit.pyFiles\":\"python.zip\",\n",
    "}\n",
    "spark = ms.spark.get_session(local=False,\n",
    "                             app_name='soc-pokec ng sampling',\n",
    "                             batch_size=128,\n",
    "                             worker_count=2,\n",
    "                             server_count=2,\n",
    "                             worker_memory='10G',\n",
    "                             server_memory='10G',\n",
    "                             coordinator_memory='10G',\n",
    "                             spark_confs=spark_confs)\n",
    "sc = spark.sparkContext\n",
    "print('Debug -- spark init')\n",
    "print('Debug -- version:', sc.version)   \n",
    "print('Debug -- applicaitonId:', sc.applicationId)\n",
    "print('Debug -- uiWebUrl:', sc.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2a534-6a1f-4ad9-8356-84823ed467e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "train_dataset = spark.read.parquet('s3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_fg/train_dataset.parquet')\n",
    "test_dataset = spark.read.parquet('s3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_fg/test_dataset.parquet')\n",
    "item_dataset = spark.read.parquet('s3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_fg/item_dataset.parquet')\n",
    "\n",
    "\n",
    "all_dataset = train_dataset.union(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb619f-890e-4a25-8a7d-97c51f5c2c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.neg_sampler import negative_sampling\n",
    "\n",
    "neg_sample_df = negative_sampling(spark, dataset=all_dataset, user_column='user_id', item_column='friend_id', time_column=None, \n",
    "                                      negative_item_column='trigger_item_id', negative_sample=3)\n",
    "neg_sample_df.cache()\n",
    "\n",
    "print('count of neg_sample_df: ', neg_sample_df.count())\n",
    "neg_sample_df.limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378ed737-3b83-44cd-910f-d1b8ff1f6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sample_df = neg_sample_df.withColumn('label', F.lit('0'))\n",
    "\n",
    "neg_sample_df = neg_sample_df.alias('t1')\\\n",
    "                            .join(all_dataset.alias('t2'), \\\n",
    "                                (F.col('t1.user_id')==F.col('t2.user_id')) & (F.col('t1.trigger_item_id')==F.col('t2.friend_id')),\n",
    "                                how='leftouter')\\\n",
    "                            .select('t1.label', \\\n",
    "                                't1.user_id', 't1.friend_id', 't2.user_gender', 't2.user_age', 't2.user_completion_percentage')\n",
    "\n",
    "neg_sample_df = neg_sample_df.alias('t1')\\\n",
    "                            .join(item_dataset.alias('t2'), \\\n",
    "                                F.col('t1.friend_id')==F.col('t2.friend_id'),\n",
    "                                how='leftouter')\\\n",
    "                            .select('t1.*', 't2.friend_gender', 't2.friend_age', 't2.friend_completion_percentage')\n",
    "\n",
    "neg_sample_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab41ff-645b-424a-8422-907f8bb38a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = all_dataset.union(neg_sample_df)\n",
    "\n",
    "splits = all_dataset.randomSplit([0.9, 0.1], 24)\n",
    "train_dataset_rank, test_dataset_rank = splits[0], splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806fb59-f3ad-4f5d-bb29-6fc4b18eee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_rank.cache()\n",
    "test_dataset_rank.cache()\n",
    "\n",
    "print('Percentage of positive sample in train_dataset: ', train_dataset_rank.filter(F.col('label') == '1').count() / train_dataset_rank.count())\n",
    "print('Percentage of positive sample in test_dataset: ', test_dataset_rank.filter(F.col('label') == '1').count() / test_dataset_rank.count())\n",
    "\n",
    "train_dataset_rank.write.parquet('s3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_fg/train_dataset_rank.parquet', mode=\"overwrite\")\n",
    "test_dataset_rank.write.parquet('s3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_fg/test_dataset_rank.parquet', mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4237540-b791-4c29-bb2e-931281d975e8",
   "metadata": {},
   "source": [
    "# DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6e892-17e6-4fe8-b6fe-2138a3b7e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import subprocess\n",
    "import sys\n",
    "import metaspore as ms\n",
    "\n",
    "model_params = dict()\n",
    "with open('conf/soc_pokec_deepfm.yaml', 'r') as stream:\n",
    "    model_params = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "    print('Debug -- load config: ', model_params)\n",
    "    \n",
    "locals().update(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2525f8e-9b83-4f3b-8306-b41e0f91798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(['zip', '-r', 'solutions/recommend/offline/social_network/python.zip', 'python'], cwd='../../../../')\n",
    "spark_confs={\n",
    "    \"spark.network.timeout\":\"500\",\n",
    "    \"spark.ui.showConsoleProgress\": \"true\",\n",
    "    \"spark.kubernetes.executor.deleteOnTermination\":\"true\",\n",
    "    \"spark.submit.pyFiles\":\"python.zip\",\n",
    "}\n",
    "spark = ms.spark.get_session(local=model_params['local'],\n",
    "                             app_name=model_params['app_name'],\n",
    "                             batch_size=model_params['batch_size'],\n",
    "                             worker_count=model_params['worker_count'],\n",
    "                             server_count=model_params['server_count'],\n",
    "                             worker_memory=model_params['worker_memory'],\n",
    "                             server_memory=model_params['server_memory'],\n",
    "                             coordinator_memory=model_params['coordinator_memory'],\n",
    "                             spark_confs=spark_confs)\n",
    "sc = spark.sparkContext\n",
    "print('Debug -- spark init')\n",
    "print('Debug -- version:', sc.version)   \n",
    "print('Debug -- applicaitonId:', sc.applicationId)\n",
    "print('Debug -- uiWebUrl:', sc.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a9c0d-efc8-452d-8fa0-1ca56a92319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "train_dataset = spark.read.parquet(model_params['train_path'])\n",
    "test_dataset = spark.read.parquet(model_params['test_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ab383-53b9-4af0-90fe-9d6d61e808ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from python.algos.deepfm_net import DeepFM\n",
    "\n",
    "module = DeepFM(use_wide=use_wide,\n",
    "            use_dnn=use_dnn,\n",
    "            use_fm=use_fm,\n",
    "            wide_embedding_dim=wide_embedding_dim,\n",
    "            deep_embedding_dim=deep_embedding_dim,\n",
    "            wide_column_name_path=wide_column_name_path,\n",
    "            wide_combine_schema_path=wide_combine_schema_path,\n",
    "            deep_column_name_path=deep_column_name_path,\n",
    "            deep_combine_schema_path=deep_combine_schema_path,\n",
    "            sparse_init_var=sparse_init_var,\n",
    "            dnn_hidden_units=dnn_hidden_units,\n",
    "            dnn_hidden_activations=dnn_hidden_activations,\n",
    "            use_bias=use_bias,\n",
    "            batch_norm=batch_norm,\n",
    "            net_dropout=net_dropout,\n",
    "            net_regularizer=net_regularizer,\n",
    "            ftrl_l1=ftrl_l1,\n",
    "            ftrl_l2=ftrl_l2,\n",
    "            ftrl_alpha=ftrl_alpha,\n",
    "            ftrl_beta=ftrl_beta)\n",
    "\n",
    "estimator = ms.PyTorchEstimator(module=module, **model_params)\n",
    "\n",
    "estimator.updater = ms.AdamTensorUpdater(adam_learning_rate)\n",
    "model = estimator.fit(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1299ca1-719f-48fc-b12d-7f70da0b1442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "train_result = model.transform(train_dataset)\n",
    "test_result = model.transform(test_dataset)\n",
    "\n",
    "train_evaluator = BinaryClassificationEvaluator()\n",
    "train_auc = train_evaluator.evaluate(train_result)\n",
    "\n",
    "test_evaluator = BinaryClassificationEvaluator()\n",
    "test_auc = test_evaluator.evaluate(test_result)\n",
    "\n",
    "print('Debug -- Train AUC: ', train_auc)\n",
    "print('Debug -- Test AUC: ', test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5903f-5115-444b-8aa6-0e65c0d2387e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d73c8620-648a-4a07-93a4-198d1edcb37b",
   "metadata": {},
   "source": [
    "# Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f8b91-8f66-44fc-921b-35b49912c821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug -- load config:  {'app_name': 'Pipeline Demo', 'local': False, 'worker_count': 2, 'server_count': 2, 'batch_size': 128, 'worker_memory': '4G', 'server_memory': '4G', 'coordinator_memory': '4G', 'zip_path': '../MetaSpore/solutions/recommend/offline/social_network/python.zip', 'zip_cwd': '../../../../../recommend-algos', 'train_path': 's3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_fg/train_dataset.parquet', 'test_path': 's3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_fg/test_dataset.parquet', 'item_path': 's3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_fg/item_dataset.parquet', 'user_id': 'user_id', 'item_id': 'friend_id', 'two_tower_module': 'metaspore', 'two_tower_module_class': 'TwoTowerBatchNegativeSamplingModule', 'two_tower_estimator_class': 'TwoTowerRetrievalEstimator', 'user_column_name': 's3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_schema/column_name.txt', 'user_combine_schema': 's3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_schema/user_combine_schema.txt', 'item_column_name': 's3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_schema/column_name.txt', 'item_combine_schema': 's3://dmetasoul-bucket/demo/datasets/soc-pokec/demo_schema/item_combine_schema.txt', 'model_in_path': None, 'model_out_path': 's3://dmetasoul-bucket/demo/soc-pokec/model/dssm/model_out/', 'model_export_path': None, 'model_version': '0.1', 'experiment_name': 'soc_pokec_two_towers_dssm', 'milvus_description': 'ml_1m_dssm', 'milvus_host': 'my-milvus-release.milvus.svc.cluster.local', 'milvus_port': '19530', 'milvus_embedding_field': 'embedding_vector', 'milvus_index_type': 'IVF_FLAT', 'milvus_metric_type': 'IP', 'milvus_nlist': 1024, 'milvus_nprobe': 128, 'input_label_column_index': 0, 'input_feature_column_num': 8, 'input_item_id_column_index': 2, 'input_item_probability_column_index': -1, 'input_sample_weight_column_index': -1, 'tau': 0.05, 'use_remove_accidental_hits': True, 'use_sampling_probability_correction': False, 'use_sample_weight': False, 'vector_embedding_size': 16, 'item_embedding_size': 64, 'net_dropout': 0.0, 'dense_structure': [256, 128, 64], 'adam_learning_rate': 1e-05, 'ftrl_learning_rate': 0.02, 'ftrl_smothing_rate': 1.0, 'ftrl_l1_regularization': 1.0, 'ftrl_l2_regularization': 1.0, 'training_epoches': 1, 'shuffle_training_dataset': True}\n",
      "updating: python/ (stored 0%)\n",
      "updating: python/dssm_inbatch_net.py (deflated 76%)\n",
      "updating: python/xdeepfm_net.py (deflated 76%)\n",
      "updating: python/cf_retrieval.py (deflated 80%)\n",
      "updating: python/widedeep_net.py (deflated 74%)\n",
      "updating: python/training/ (stored 0%)\n",
      "updating: python/training/multitask_esmm_learning.py (deflated 76%)\n",
      "updating: python/training/__init__.py (deflated 41%)\n",
      "updating: python/training/__pycache__/ (stored 0%)\n",
      "updating: python/training/__pycache__/listwise.cpython-38.pyc (deflated 55%)\n",
      "updating: python/training/__pycache__/__init__.cpython-38.pyc (deflated 26%)\n",
      "updating: python/training/__pycache__/multitask_esmm_learning.cpython-38.pyc (deflated 54%)\n",
      "updating: python/training/__pycache__/two_tower_contrastive_learning.cpython-38.pyc (deflated 55%)\n",
      "updating: python/training/__pycache__/batch_negative_sampling.cpython-38.pyc (deflated 53%)\n",
      "updating: python/training/__pycache__/retrieval_metric.cpython-38.pyc (deflated 51%)\n",
      "updating: python/training/.ipynb_checkpoints/ (stored 0%)\n",
      "updating: python/training/.ipynb_checkpoints/batch_negative_sampling-checkpoint.py (deflated 73%)\n",
      "updating: python/training/.ipynb_checkpoints/multitask_esmm_learning-checkpoint.py (deflated 76%)\n",
      "updating: python/training/.ipynb_checkpoints/two_tower_contrastive_learning-checkpoint.py (deflated 74%)\n",
      "updating: python/training/.ipynb_checkpoints/retrieval_metric-checkpoint.py (deflated 73%)\n",
      "updating: python/training/.ipynb_checkpoints/__init__-checkpoint.py (deflated 43%)\n",
      "updating: python/training/retrieval_metric.py (deflated 73%)\n",
      "updating: python/training/batch_negative_sampling.py (deflated 73%)\n",
      "updating: python/tuner/ (stored 0%)\n",
      "updating: python/tuner/nohup.out (deflated 85%)\n",
      "updating: python/tuner/base_tuner.py (deflated 73%)\n",
      "updating: python/tuner/__pycache__/ (stored 0%)\n",
      "updating: python/tuner/__pycache__/base_tuner.cpython-38.pyc (deflated 47%)\n",
      "updating: python/tuner/.ipynb_checkpoints/ (stored 0%)\n",
      "updating: python/tuner/.ipynb_checkpoints/simplex_tuner-checkpoint.py (deflated 77%)\n",
      "updating: python/tuner/.ipynb_checkpoints/base_tuner-checkpoint.py (deflated 71%)\n",
      "updating: python/tuner/.ipynb_checkpoints/dssm_tuner-checkpoint.py (deflated 76%)\n",
      "updating: python/tuner/simplex_tuner.py (deflated 77%)\n",
      "updating: python/tuner/dssm_tuner.py (deflated 76%)\n",
      "updating: python/tuner/python.zip (stored 0%)\n",
      "updating: python/multitask/ (stored 0%)\n",
      "updating: python/multitask/mmoe_net.py (deflated 79%)\n",
      "updating: python/multitask/mmoe_agent.py (deflated 73%)\n",
      "updating: python/multitask/__pycache__/ (stored 0%)\n",
      "updating: python/multitask/__pycache__/mmoe_net.cpython-38.pyc (deflated 43%)\n",
      "updating: python/multitask/__pycache__/mmoe_agent.cpython-38.pyc (deflated 52%)\n",
      "updating: python/multitask/.ipynb_checkpoints/ (stored 0%)\n",
      "updating: python/multitask/.ipynb_checkpoints/mmoe_net-checkpoint.py (deflated 75%)\n",
      "updating: python/multitask/.ipynb_checkpoints/mmoe_agent-checkpoint.py (deflated 70%)\n",
      "updating: python/item_cf_retrieval.py (deflated 79%)\n",
      "updating: python/esmm_net.py (deflated 75%)\n",
      "updating: python/pnn_net.py (deflated 74%)\n",
      "updating: python/tools.py (deflated 75%)\n",
      "updating: python/__init__.py (deflated 38%)\n",
      "updating: python/dlrm_net.py (deflated 72%)\n",
      "updating: python/dcn_v2_net.py (deflated 79%)\n",
      "updating: python/__pycache__/ (stored 0%)\n",
      "updating: python/__pycache__/widedeep_net.cpython-38.pyc (deflated 42%)\n",
      "updating: python/__pycache__/two_tower_retrieval_milvus.cpython-38.pyc (deflated 60%)\n",
      "updating: python/__pycache__/__init__.cpython-38.pyc (deflated 22%)\n",
      "updating: python/__pycache__/autoint_net.cpython-38.pyc (deflated 40%)\n",
      "updating: python/__pycache__/dssm_net.cpython-38.pyc (deflated 55%)\n",
      "updating: python/__pycache__/layers.cpython-38.pyc (deflated 59%)\n",
      "updating: python/__pycache__/dcn_net.cpython-38.pyc (deflated 40%)\n",
      "updating: python/__pycache__/xdeepfm_net.cpython-38.pyc (deflated 42%)\n",
      "updating: python/__pycache__/dcn_v2_net.cpython-38.pyc (deflated 43%)\n",
      "updating: python/__pycache__/deepfm_net.cpython-38.pyc (deflated 42%)\n",
      "updating: python/__pycache__/pnn_net.cpython-38.pyc (deflated 43%)\n",
      "updating: python/__pycache__/dlrm_net.cpython-38.pyc (deflated 42%)\n",
      "updating: python/.ipynb_checkpoints/ (stored 0%)\n",
      "updating: python/.ipynb_checkpoints/two_tower_retrieval_milvus-checkpoint.py (deflated 82%)\n",
      "updating: python/.ipynb_checkpoints/cf_retrieval-checkpoint.py (deflated 80%)\n",
      "updating: python/.ipynb_checkpoints/autoint_net-checkpoint.py (deflated 74%)\n",
      "updating: python/.ipynb_checkpoints/pnn_net-checkpoint.py (deflated 74%)\n",
      "updating: python/.ipynb_checkpoints/layers-checkpoint.py (deflated 79%)\n",
      "updating: python/.ipynb_checkpoints/nohup-checkpoint.out (deflated 87%)\n",
      "updating: python/.ipynb_checkpoints/widedeep-checkpoint.py (deflated 71%)\n",
      "updating: python/.ipynb_checkpoints/dlrm_net-checkpoint.py (deflated 72%)\n",
      "updating: python/.ipynb_checkpoints/dcn_net-checkpoint.py (deflated 74%)\n",
      "updating: python/.ipynb_checkpoints/xdeepfm_net-checkpoint.py (deflated 76%)\n",
      "updating: python/.ipynb_checkpoints/dssm_net-checkpoint.py (deflated 82%)\n",
      "updating: python/.ipynb_checkpoints/milvus_agents-checkpoint.py (deflated 77%)\n",
      "updating: python/.ipynb_checkpoints/deepfm_net-checkpoint.py (deflated 74%)\n",
      "updating: python/.ipynb_checkpoints/dcn_v2_net-checkpoint.py (deflated 79%)\n",
      "updating: python/.ipynb_checkpoints/widedeep_net-checkpoint.py (deflated 74%)\n",
      "updating: python/.ipynb_checkpoints/esmm_net-checkpoint.py (deflated 75%)\n",
      "updating: python/.ipynb_checkpoints/__init__-checkpoint.py (deflated 51%)\n",
      "updating: python/.ipynb_checkpoints/tools-checkpoint.py (deflated 75%)\n",
      "updating: python/dcn_net.py (deflated 74%)\n",
      "updating: python/softmax_loss/ (stored 0%)\n",
      "updating: python/softmax_loss/__pycache__/ (stored 0%)\n",
      "updating: python/softmax_loss/__pycache__/two_tower_retrieval_milvus.cpython-38.pyc (deflated 60%)\n",
      "updating: python/softmax_loss/.ipynb_checkpoints/ (stored 0%)\n",
      "updating: python/softmax_loss/.ipynb_checkpoints/two_tower_retrieval_milvus-checkpoint.py (deflated 82%)\n",
      "updating: python/softmax_loss/simplex/ (stored 0%)\n",
      "updating: python/softmax_loss/simplex/__pycache__/ (stored 0%)\n",
      "updating: python/softmax_loss/simplex/__pycache__/two_tower_retrieval.cpython-38.pyc (deflated 60%)\n",
      "updating: python/softmax_loss/simplex/__pycache__/simplex_metric.cpython-38.pyc (deflated 52%)\n",
      "updating: python/softmax_loss/simplex/__pycache__/simplex_agent.cpython-38.pyc (deflated 51%)\n",
      "updating: python/softmax_loss/simplex/__pycache__/simplex_net.cpython-38.pyc (deflated 54%)\n",
      "updating: python/softmax_loss/simplex/.ipynb_checkpoints/ (stored 0%)\n",
      "updating: python/softmax_loss/simplex/.ipynb_checkpoints/simplex_net-checkpoint.py (deflated 77%)\n",
      "updating: python/softmax_loss/simplex/.ipynb_checkpoints/simplex_metric-checkpoint.py (deflated 73%)\n",
      "updating: python/softmax_loss/simplex/.ipynb_checkpoints/simplex_agent-checkpoint.py (deflated 71%)\n",
      "updating: python/softmax_loss/simplex/.ipynb_checkpoints/two_tower_retrieval-checkpoint.py (deflated 81%)\n",
      "updating: python/milvus_agents.py (deflated 77%)\n",
      "updating: python/dssm_net.py (deflated 78%)\n",
      "updating: python/autoint_net.py (deflated 74%)\n",
      "updating: python/two_tower_retrieval_milvus.py (deflated 82%)\n",
      "updating: python/simplex/ (stored 0%)\n",
      "updating: python/simplex/simplex_net.py (deflated 76%)\n",
      "updating: python/simplex/simplex_agent.py (deflated 69%)\n",
      "updating: python/simplex/__pycache__/ (stored 0%)\n",
      "updating: python/simplex/__pycache__/two_tower_retrieval.cpython-38.pyc (deflated 60%)\n",
      "updating: python/simplex/__pycache__/simplex_metric.cpython-38.pyc (deflated 51%)\n",
      "updating: python/simplex/__pycache__/simplex_agent.cpython-38.pyc (deflated 47%)\n",
      "updating: python/simplex/__pycache__/simplex_net.cpython-38.pyc (deflated 53%)\n",
      "updating: python/simplex/.ipynb_checkpoints/ (stored 0%)\n",
      "updating: python/simplex/.ipynb_checkpoints/simplex_net-checkpoint.py (deflated 77%)\n",
      "updating: python/simplex/.ipynb_checkpoints/simplex_metric-checkpoint.py (deflated 73%)\n",
      "updating: python/simplex/.ipynb_checkpoints/simplex_agent-checkpoint.py (deflated 69%)\n",
      "updating: python/simplex/.ipynb_checkpoints/two_tower_retrieval-checkpoint.py (deflated 81%)\n",
      "updating: python/simplex/simplex_metric.py (deflated 73%)\n",
      "updating: python/deepfm_net.py (deflated 74%)\n",
      "updating: python/layers.py (deflated 79%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/06/22 10:46:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/22 10:46:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pipelines.pipeline import Pipeline\n",
    "from pipelines.nodes.init_spark import InitSparkNode\n",
    "from pipelines.nodes.data_loader import DataLoaderNode\n",
    "from pipelines.nodes.two_towers_estimator import TwoTowersEstimatorNode\n",
    "from pipelines.nodes.retrieval_evaluator import RetrievalEvaluatorNode\n",
    "\n",
    "p = Pipeline('pipelines/test.yaml')\n",
    "p.add_node(InitSparkNode())\n",
    "p.add_node(DataLoaderNode())\n",
    "p.add_node(TwoTowersEstimatorNode())\n",
    "p.add_node(RetrievalEvaluatorNode())\n",
    "\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57656218-e97b-47e2-8193-0df37e01afbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02307335-ff28-47ff-943c-69be47e465c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
